# RAG Chatbot using LangChain

## ğŸ“Œ Project Overview
This project is an implementation of a **Retrieval-Augmented Generation (RAG) chatbot** using **LangChain**, designed to efficiently retrieve relevant context from uploaded PDFs and generate meaningful responses based on user queries. The chatbot leverages **Large Language Models (LLMs)** to enhance knowledge retrieval and provide accurate, context-aware answers.

## ğŸ¯ Purpose of the Project
This chatbot was built as part of my learning journey into **RAG-based architectures**, **LangChain**, and **LLM-powered applications**. The goal was to:
- Understand **document-based Q&A systems** using embeddings and vector stores.
- Learn how to integrate **LLMs with external knowledge sources** for better contextual awareness.
- Explore **LangChain** and its components to build practical AI-driven applications.
- Experiment with **FastAPI** for the backend and **Gradio** for an intuitive frontend.

## âš™ï¸ Features
âœ” **PDF Upload Support** â€“ Users can upload PDF files, and the chatbot retrieves relevant context from them.
âœ” **RAG-Powered Responses** â€“ Uses LangChain to fetch relevant information from PDFs before generating answers.
âœ” **Gradio Frontend** â€“ A simple and interactive UI for seamless interactions.
âœ” **FastAPI Backend** â€“ Ensures efficient request handling and model predictions.
âœ” **Scalable Architecture** â€“ Can be extended with different LLMs and vector stores.

## ğŸš€ Tech Stack
- **LangChain** â€“ Framework for building LLM applications with knowledge retrieval.
- **OpenAI/LLM Models** â€“ For generating responses based on retrieved context.
- **FAISS / ChromaDB** â€“ Used as vector stores to store and retrieve embeddings.
- **FastAPI** â€“ For handling API requests efficiently.
- **Gradio** â€“ Provides a clean and easy-to-use interface for user interaction.
- **Python** â€“ The core programming language for the implementation.

## ğŸ“‚ Project Structure
```
RAG_Chatbot/
â”‚â”€â”€ main.py                 # Backend API using FastAPI
â”‚â”€â”€ gradio_frontend.py      # Gradio-based UI
â”‚â”€â”€ # CHATBOT MODULE.py     # Handles chatbot logic
â”‚â”€â”€ # MEMORY MANAGER MODULE.py  # Manages context retention
â”‚â”€â”€ # PDF PROCESSING MODULE.py  # Extracts text from PDFs
â”‚â”€â”€ # UTILS MODULE.py       # Helper functions
â”‚â”€â”€ # MODEL SETUP MODULE.py # Loads and configures the LLM
â”‚â”€â”€ requirements.txt        # List of dependencies
```

## ğŸ› ï¸ Installation & Usage
### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/KIREN2612/RAG_Chatbot-using-Langchain.git
cd RAG_Chatbot-using-Langchain
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the Backend API
```bash
python main.py
```

### 4ï¸âƒ£ Run the Frontend
```bash
python gradio_frontend.py
```

### 5ï¸âƒ£ Open the Gradio UI and Start Chatting!
After running the frontend script, Gradio will provide a **local URL** where you can upload PDFs and interact with the chatbot.

## ğŸ“Œ Future Enhancements
ğŸ”¹ Improve response accuracy by integrating **more advanced vector databases**.<br>
ğŸ”¹ Enable support for **multiple document formats (Word, TXT, etc.)**.<br>
ğŸ”¹ Add **user authentication and session-based memory**.<br>
ğŸ”¹ Explore **fine-tuned models** for domain-specific applications.<br>

## ğŸ“œ License
This project is open-source under the **Apache license 2.0**.

---
ğŸ’¡ **This project is a part of my ongoing exploration into LLM-based applications.** If you find it interesting, feel free to fork, contribute, or share feedback! ğŸš€

